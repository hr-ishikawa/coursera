R predict next word
Predict Next Word - RPubs
http://www.rpubs.com/chosengmong/Final_Capstone_Project


Katz-backoffモデル
Good-Turing スムージング

■言語モデル入門 （第二版） - SlideShare
https://www.slideshare.net/yoshinarifujinuma/ss-40451841
◆Kneser-neyスムージング：overview
 •実験的に一番良いスムージング 
 •いくつかバリエーションがある
  – Interpolated Kneser-ney (Kneser & Ney 1995) 
 •今回はこっちを説明
  – Modified Kneser-ney(Chen & Goodman 1999) 
 •アイディアは： –直前の単語の種類数を重視
◆Kneser-neyスムージング：例 
 •Bigram言語モデルを想定 
 •I want to go to Toyama Fransisco
  – Fransiscoは頻度が高いが、ほぼSanの後に続く 
 •スムージング：unigramの頻度
  – P(Toyama Fransisco) ≒ P(Fransisco)
  – P(Toyama Fransisco)が高くなってしまう！ 
 •Kneser-neyのアイディア： –P_continuation: 単語wは直前の単語の種類は 豊富か？
◆パープレキシティ

■[翻訳] text2vec vignette: text2vecパッケージによるテキスト分析
https://qiita.com/nakamichi/items/1bf2ee393cb407d9fb74


Ngram言語モデルメモ
https://jetbead.hatenablog.com/entry/20111031/1320078059
未出現事象の出現確率
https://www.slideshare.net/hirsoshnakagawa3/smoothing1-40918238
言語モデル
http://chasen.org/~daiti-m/paper/naist-dlec2004-lmodel.pdf

Rのtext2vecで次の単語を予測する
https://codeday.me/jp/qa/20190821/1494469.html
単語予測アルゴリズム
https://codeday.me/jp/qa/20190220/270194.html

install.packages("hash")
library(hash)
h <- hash()
h['banana'] <- "banana"
h
h[['banana']] # [1] "banana"

install.packages("openssl")
library(openssl)
sha256("banana") # [1] "b493d48364afe44d11c0165cf470a4164d1e2609911ef998be868d46ade3de4e"

3.3 N-gram モデルのスムージング
ただし，N-gramモデルにおいてはゼロ頻度問題という
問題が存在する．これは、出現頻度により N-gram 確
率を推定すると，学習用データセット中に出現しない単語
組の確率値を 0 としてしまうという問題である．これを
解決するために，線形補間法 というスムージング手法
を用いる．線形補間法は，N-gram 確率 P(wn|w
n−1
n−N+1)
を低次の M-gram(M < N) の確率値と線形に補間する
方法である．N = 2（バイグラム）の場合は，N-gram
確率は次のようになる．
P(wn|wn−1)=λP(wn|wn−1)+(1 − λ)P(wn) (2)
ここで，λ は補間係数であり，経験的に λ = 0.7 と設定す
る．また，N = 3（トライグラム）の場合には，N-gram
確率を次のように書きなおすことができる．
P(wn|wn−2wn−1) = λ3P(wn|wn−2wn−1)
+ λ2P(wn|wn−1)
+ λ1P(wn) (3)
ここで，λ3, λ2, λ1 は，それぞれトライグラム，バイグラ
ム，ユニグラムに対する補間係数であり，∑iλi = 1 と
なるように設定される．これらの補間係数に関しては，
経験的に λ3 = 0.5, λ2 = 0.3, λ1 = 0.2 と設定する．以
降，スムージング手法についてはこれらの補間係数を用
いることとする．


精度予測　データの中からランダムに抽出

