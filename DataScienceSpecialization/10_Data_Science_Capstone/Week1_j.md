## 1週目

## プロジェクトの概要

世界中で、人々は電子メール、ソーシャルネットワーキング、バンキング、その他のさまざまな活動のためにモバイルデバイスに費やす時間を増やしています。しかし、モバイルデバイスでの入力は深刻な苦痛になります。この絶頂期の企業パートナーであるSwiftKeyは、モバイルデバイスでの入力を容易にするスマートキーボードを構築します。彼らのスマートキーボードの基盤の1つは、予測テキストモデルです。誰かがタイプするとき：

I went to the

キーボードには、次の単語が何であるかについて3つのオプションがあります。たとえば、3つの単語は、ジム、店舗、レストランです。このキャップストーンでは、SwiftKeyで使用されるような予測テキストモデルの理解と構築に取り組みます。

このコースは基本から始まり、テキストドキュメントの大規模なコーパスを分析して、データの構造と単語の組み立て方を見つけます。テキストデータのクリーニングと分析、予測テキストモデルからの構築とサンプリングについて説明します。最後に、データ製品で得た知識を使用して、家族、友人、潜在的な雇用主に見せることができる予測テキスト製品を構築します。

このコースのデータサイエンススペシャライゼーションで学んだすべてのスキルを使用しますが、テキストデータの分析と自然言語処理というまったく新しいアプリケーションに取り組んでいることがわかります。この選択は意図的です。実践的なデータサイエンティストとして、新しいデータタイプと問題に頻繁に直面します。データサイエンティストとしての楽しさと挑戦の大部分は、これらの新しいデータタイプを使用して、人々が愛するデータ製品を構築する方法を見つけ出すことです。キャップストーンは、次の評価に基づいて評価されます。

1. データをダウンロードして操作できるかどうかをテストする入門クイズ。
2. コースデータセットの探索的分析を平易な言葉で記述し、プロットし、コード化する中間Rマークダウンレポート。
3. 2つの自然言語処理クイズ。予測モデルを実際のデータに適用して、その動作を確認します。
4. 入力としてフレーズ（複数の単語）を受け取り、1回のクリックで送信し、次の単語を予測する光沢のあるアプリ。
5. Rプレゼンテーションで作成された5つのスライドデッキで、アルゴリズムとアプリを上司または投資家に売り込みます。

絶頂期には、仲間の学生、私たち、SwiftKeyのエンジニアからサポートを受けることができます。しかし、私たちは本当にあなたがあなたの独立性、創造性、そしてイニシアチブを示すことを望んでいます。これまでのクラスでのパフォーマンスに非常に感銘を受けており、素晴らしいことができることを知っています。

基本的な自然言語処理リソースを以下にまとめました。これらのリソース、またはこの分析の実行中に見つけられる他のリソースを使用してください。心に留めておくべきことの1つは、**自然言語処理の世界的な専門家になることを期待していないことです。**このキャップストーンのポイントは、新しいデータ型を探索し、迅速にスピードアップできることを示すこと新しいアプリケーションで、合理的な期間内に有用なモデルを実装します。 NLPは非常にクールであり、将来の目標に応じてさらに深く勉強する価値があると思いますが、データサイエンスの一般的な知識とNLPの基本的な知識を使用してこのプロジェクトを完了することができます。

以下に、この野心的なプロジェクトに取り組む際に開始するのに適したリソースをいくつか紹介します。

- Rのテキストマイニングインフラストラクチャ(http://www.jstatsoft.org/v25/i05/)  
- CRANタスクビュー：自然言語処理(http://cran.r-project.org/web/views/NaturalLanguageProcessing.html)  
- スタンフォード自然言語処理コースの動画(https://www.youtube.com/user/OpenCourseOnline/search?query=NLP) およびスライド(https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html)  

## Syllbus (シラバス、概要)
### コースタイトル
Data Science Specialization SwiftKey Capstone

### コースインストラクター
- ジェフ・リーク
- ロジャー・ペン
- ブライアンカッフォ

### あなたは今、データサイエンティストです
このデータサイエンス専門分野の目標は、データサイエンティストになるための基本的なスキルを身に付けることです。このキャップストーンの目標は、データサイエンティストであるという経験を模倣することです。実践的なデータサイエンティストとして、散らかったデータセット、あいまいな質問、およびデータを正確に分析する方法に関するごくわずかな指示を取得することは完全に一般的です。私たちの目標は、フォーラム、インストラクターとの話し合い、SwiftKeyとCourseraのエンジニアからのフィードバック、解決する構造化された問題という形で追加のサポートを提供することです。このプロジェクトがあなたにあなたのスキルと創造性を示す機会を与えてくれることを願っています。

### コースのタスク
このコースは、実践的なデータサイエンティストが遭遇するさまざまなアクティビティをカバーする8つの異なるタスクに分けられます。これらは、データサイエンスの専門分野で開発したスキルの多くを反映しています。タスクは次のとおりです。

- 問題を理解する
- データの取得とクリーニング
- 探索的分析
- 統計モデリング
- 予測モデリング
- 創造的な探求
- データ製品の作成
- 製品を売り込む短いスライドデッキの作成

キャップストーンの過程でこれらの各タスクについて聞くことができます。

### コースデータセット
これは、大部分のキャップストーンの基礎となる、開始するためのトレーニングデータです。開始するには、外部Webサイトからではなく、以下のリンクからデータをダウンロードする必要があります。
- https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

コースの後半で、外部データセットを使用して、適切と思われるモデルを拡張できます。

### 意見の相違
現在、データ分析は科学と同じくらい芸術的であることに留意してください（したがって、意見の相違があるかもしれません）。掲示板への怒り、皮肉、虐待的なコメントは控えてください。私たちの目標は、最も上級者からこの教材を初めて見た人まで、すべての学生の学習を支援する支援コミュニティを作成することです。

## Task 0-問題を理解する

新しいデータセットを分析する最初のステップは、（a）所有しているデータと（b）そのタイプのデータに使用されている標準のツールとモデルを把握することです。演習に進む前に、必ずCourseraからデータをダウンロードしてください。この演習では、LOCALE.blogs.txtという名前のファイルを使用します。LOCALEは、en_US、de_DE、ru_RU、およびfi_FIの4つのロケールのそれぞれです。データは、HC Corporaと呼ばれるコーパスからのものです。詳細については、コーパスのリーディングについてを参照してください。ファイルは言語フィルタリングされていますが、依然として外国語のテキストが含まれている場合があります。

このキャップストーンでは、自然言語処理の分野でデータサイエンスを適用します。このプロジェクトに取り組むための最初のステップとして、自然言語処理、テキストマイニング、およびRの関連ツールに精通する必要があります。ここで役立つリソースをいくつか紹介します。

- 自然言語処理ウィキペディアページ（https://en.wikipedia.org/wiki/Natural_language_processing）
- Rのテキストマイニングインフラストラクチャ（http://www.jstatsoft.org/v25/i05/）
- CRANタスクビュー：自然言語処理（http://cran.r-project.org/web/views/NaturalLanguageProcessing.html）

#### データセット

これは、大部分のキャップストーンの基礎となる、開始するためのトレーニングデータです。開始するには、外部WebサイトからではなくCourseraサイトからデータをダウンロードする必要があります。

- Capstoneデータセット（https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip）

データの元の調査およびモデリング手順は、このデータセットで実行されます。キャップストーンの後半で、モデルの構築に役立つ可能性のある追加のデータセットが見つかった場合は、それらを使用できます。

#### 達成するタスク

1.データの取得-データをダウンロードして、Rでロード/操作できますか？
2. NLPとテキストマイニングに精通する-自然言語処理の基本と、それがData Science Specializationで学習したデータサイエンスプロセスにどのように関係するかを学びます。

#### 考慮すべき質問

1. データはどのように見えますか？
2. データはどこから来ますか？
3. このプロジェクトに役立つ可能性のある他のデータソースについて考えてください。
4. 自然言語処理の一般的な手順は何ですか？
5. テキストデータの分析における一般的な問題は何ですか？
6. NLPとスペシャライゼーションで学んだ概念との関係は何ですか？

## コーパスについて

コーパスは、Webクローラーによって公開されているソースから収集されます。主に目的の言語*で構成されるテキストを取得するために、クローラーは言語をチェックします。

各エントリには、発行日がタグ付けされています。ユーザーのコメントが含まれる場合、メインエントリの日付でタグ付けされます。

各エントリには、収集元のWebサイトのタイプ（新聞や個人ブログなど）に基づいて、エントリのタイプのタグが付けられます。可能であれば、各エントリには、エントリのタイトルまたはキーワードに基づく1つ以上の件名のタグが付けられます（例、エントリが新聞のスポーツセクションからのものである場合、「スポーツ」件名でタグ付けされます。多くの場合、エントリにタグ付けすることは実行できません（たとえば、個々のTwitterエントリにタグ付けするのは実際的ではありませんが、将来実装される可能性のあるいくつかのアイデアがあります）または自動化されたプロセスでサブジェクトが見つからない場合は、エントリに「0」のタグが付けられます。

スペースを節約するために、件名とタイプは数値コードで示されます。

生のコーパスが収集されると、さらに解析され、重複エントリが削除され、個々の行に分割されます。その後、各エントリの約50％が削除されます。エントリを完全に再作成することはできないため、エントリは匿名化され、これは非営利のベンチャーであり、フェアユース（https://web-beta.archive.org/web/20160930083655/http:// en.wikipedia.org/wiki/Fair_use）。

### コーパスのサンプル

<<略>>

2016年9月30日にheliohost.orgからアーカイブされ、2017年4月24日にWayback Machineを介して取得されたコンテンツ。https://web-beta.archive.org/web/20160930083655/http://www.corpora.heliohost.org/ aboutcorpus.html

## タスク1：データの取得とクリーニング

ターゲット言語のテキストで構成される大規模なデータベースは、さまざまな目的で言語モデルを生成するときに一般的に使用されます。この演習では、**英語のデータベース**を使用しますが、ドイツ語、ロシア語、フィンランド語の他の3つのデータベースを検討する場合があります。

このタスクの目標は、データベースに精通し、必要なクリーニングを実行することです。この演習の後、実際のデータがどのように見え、データのクリーニングにどれだけの労力が必要かを理解する必要があります。新しい言語の開発を開始するとき、最初に行うことは、ターゲットとなる言語とその特性を理解することです。言語の読み方、話し方、書き方を学ぶことができます。または、データを勉強し、文学やインターネットを通じて言語に関する既存の情報から学ぶことができます。少なくとも、言語の記述方法を理解する必要があります。スクリプトの記述、既存の入力メソッド、音声知識など。

データには不快な意味のある言葉が含まれていることに注意してください。開発者が作業しなければならないという事実を強調するために、それらは意図的に残されています。

#### 達成するタスク

1. トークン化-単語、句読点、数字などの適切なトークンを識別します。ファイルを入力として受け取り、そのトークン化バージョンを返す関数を作成します。
2. 冒とく的なフィルタリング-予測したくない冒とくやその他の単語を削除します。

#### ヒント、トリック、およびヒント

1. **データをロード中**。このデータセットはかなり大きいです。アルゴリズムを構築するためにデータセット全体を読み込む必要は必ずしもないことを強調します（以下のポイント2を参照）。少なくとも最初は、データのより小さなサブセットを使用することをお勧めします。 RのreadLinesまたはスキャン関数を使用して、チャンクまたは行を読み取ると便利です。 for / whileループ内にreadLinesを埋め込むことにより、テキストの各行をループすることもできますが、これは一度に大きなチャンクで読み取るよりも時間がかかる場合があります。一度にファイルの一部を読み取るには、Rでファイル接続を使用する必要があります。たとえば、次のコードを使用して、英語のTwitterデータセットの最初の数行を読み取ることができます。con<-file（ "en_US.twitter。 txt "、" r "）readLines（con、1）##テキストの最初の行を読むreadLines（con、1）##テキストの次の行を読むreadLines（con、5）##次の5行を読むtext close（con）##完了したら、接続を閉じることが重要です。詳細については、接続のヘルプページを参照してください。
2. **サンプリング**。繰り返しますが、モデルを構築するために、すべてのデータを読み込んで使用する必要はありません。多くの場合、すべてのデータを使用して得られる結果の正確な近似を得るために、ランダムに選択された行またはチャンクを比較的少数含める必要があります。推論クラスと、代表サンプルを使用して母集団に関する事実を推測する方法を覚えておいてください。元のデータのランダムなサブセットを読み取り、それを別のファイルに書き出すことにより、別のサブサンプルデータセットを作成できます。そうすれば、サンプルを保存でき、毎回作成する必要はありません。 rbinom関数を使用して「バイアスされたコインを反転」し、テキストの行をサンプリングするかどうかを決定できます。


